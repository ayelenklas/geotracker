{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1723532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# import re\n",
    "import time\n",
    "import json\n",
    "import helium\n",
    "import chromedriver_binary\n",
    "\n",
    "# data wrangling imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# other imports\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855aa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating csv file to collect all data\n",
    "fieldnames = [\n",
    "    'restaurant_name', 'avg_review_score', 'street', 'zip_code', 'city_name',\n",
    "    'type_of_cuisine', 'minimum_order_value', 'delivery_fee', 'pricyness',\n",
    "    'latitude', 'longitude', 'avg_delivery_time']\n",
    "\n",
    "with open(\"../geotracker/data/wolt_2611.csv\", \"w\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "\n",
    "def save_in_csv(resto_info):\n",
    "    fieldnames = [\n",
    "        'restaurant_name', 'avg_review_score', 'street', 'zip_code',\n",
    "        'city_name', 'type_of_cuisine', 'minimum_order_value', 'delivery_fee',\n",
    "        'pricyness', 'latitude', 'longitude', 'avg_delivery_time']\n",
    "    with open(\"../geotracker/data/wolt_2611.csv\", \"a\", newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writerow(resto_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34551c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating headers for the csv file\n",
    "headers = {}\n",
    "for x,y in zip(fieldnames,fieldnames):\n",
    "    headers[x] = y\n",
    "    \n",
    "save_in_csv(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2e14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_codes():\n",
    "    \"\"\"method to extract and create a list with all zip codes\"\"\"\n",
    "    filename = '../raw_data/Berlin Zip Codes - Sheet1.csv'\n",
    "    zip_codes_list = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        datareader = csv.reader(csvfile)\n",
    "        for row in datareader:\n",
    "            zip_codes_list.append(row[1])\n",
    "    return zip_codes_list[1:]\n",
    "\n",
    "# list with all Berlin zipcodes\n",
    "zip_code_list = zip_codes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b027328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def links(url):\n",
    "    \"\"\"gets all links and scrape restaurant links\"\"\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    wait = WebDriverWait(driver, 15).until(ec.presence_of_element_located((By.XPATH, '/html/body/div[1]/div/div/div[2]/div[2]/div/div/div/div[2]/div[1]')))\n",
    "\n",
    "    ####  Scroller\n",
    "    SCROLL_PAUSE_TIME = 1.5\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # list that will contain all links\n",
    "    links_list = []\n",
    "\n",
    "    links = driver.find_elements_by_class_name(\"VenueVerticalList__Grid-sc-1w1a9dr-1\")[0]\n",
    "    elems = links.find_elements_by_xpath(\"//a[@href]\")\n",
    "\n",
    "    for link in elems:\n",
    "        links_list.append(link.get_attribute('href'))\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # cleaning None values from links_list\n",
    "    links_list = [x for x in links_list if x != None]\n",
    "\n",
    "    # cleaning links_list from non-restaurant profile links\n",
    "    clean_link_root = 'https://wolt.com/en/deu/berlin/restaurant/'\n",
    "    clean_links_list = [x for x in links_list if x[:42] == clean_link_root]\n",
    "\n",
    "    return clean_links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a933472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over zip_codes\n",
    "def all_links_by_zipcode(zipcode_list):\n",
    "    \"\"\"iterates over all zipcodes and creates a dictionary with zipcode num : all links\"\"\"\n",
    "    \n",
    "    base_url = \"https://wolt.com/en/discovery?tab=restaurants\"\n",
    "    \n",
    "    links_by_zip_code = {}\n",
    "\n",
    "    for zipcode in zipcode_list:\n",
    "        links_by_zip_code[zipcode] = links(f\"{base_url}&search?q={zipcode}\")\n",
    "    \n",
    "    return links_by_zip_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f6e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(page):\n",
    "    response = requests.get(page, headers={\"Accept-Language\":\"en-US\"} )\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    restaurant_info = {}\n",
    "\n",
    "    # Restaurant Name (MUST): restaurant_name\n",
    "    try:\n",
    "        restaurant_info['restaurant_name'] = soup.find(class_=\"VenueHeroBanner__TitleSpan-sc-3gkm9v-2 ifxphB\").text\n",
    "    except Exception:\n",
    "        restaurant_info['restaurant_name'] = None\n",
    "\n",
    "    # Number of Reviews (MUST) : reviews Average Review score (out of 5) (MUST) : avg_review_score - out of 10\n",
    "    try:\n",
    "        restaurant_info['avg_review_score'] = soup.find(class_=\"RatingsButton-module__score___fTqMn\").text\n",
    "    except Exception:\n",
    "        restaurant_info['avg_review_score'] = None\n",
    "\n",
    "    # Street and House Number (MUST): street\n",
    "    try:\n",
    "        restaurant_info['street'] = soup.find(class_=\"VenueSideInfo-module__primary___xK8qF\").text\n",
    "    except Exception:\n",
    "        restaurant_info['street'] = None\n",
    "\n",
    "    # ZIP code (MUST): zip_code\n",
    "    try:\n",
    "        restaurant_info['zip_code'] = soup.find(class_=\"VenueSideInfo-module__secondary___Kuira\").text[:5]\n",
    "    except Exception:\n",
    "        restaurant_info['zip_code'] = None\n",
    "    # City Name (MUST) city_name\n",
    "    try:\n",
    "        restaurant_info['city_name'] = soup.find(class_=\"VenueSideInfo-module__secondary___Kuira\").text[6:]\n",
    "    except Exception:\n",
    "        restaurant_info['zip_code'] = None\n",
    "\n",
    "\n",
    "    # Type of cuisine (SHOULD) type_of_cuisine\n",
    "    try:\n",
    "        type_of_cuisine = []\n",
    "        for elem in soup.find_all(class_=\"RelatedSearches__Item-sc-1ohvfsu-1\"):\n",
    "            tmp = elem.find('a').text\n",
    "            type_of_cuisine.append(tmp)\n",
    "        restaurant_info['type_of_cuisine'] = type_of_cuisine\n",
    "    except Exception:\n",
    "        restaurant_info['type_of_cuisine'] = None\n",
    "\n",
    "    # Minimum order value (COULD): minimum_order_value\n",
    "    # Delivery fee (COULD): delivery_fee\n",
    "\n",
    "    try:\n",
    "        tmp = soup.find_all(\n",
    "        class_=\n",
    "        'Tags__Root-sc-1dm36sr-0 ghWIPZ VenueHeroBanner__StyledConnectedTags-sc-3gkm9v-5 ljMfkO')[0].text.split(\" \")\n",
    "        # tmp[1] = tmp[1][:-4]\n",
    "        if len(tmp) ==4:\n",
    "            restaurant_info['minimum_order_value'] = tmp[3]\n",
    "            restaurant_info['delivery_fee'] = tmp[1]\n",
    "        else:\n",
    "            restaurant_info['minimum_order_value'] = tmp[2]\n",
    "            restaurant_info['delivery_fee'] = None\n",
    "    except Exception:\n",
    "        restaurant_info['minimum_order_value'] = None\n",
    "        restaurant_info['delivery_fee'] = None\n",
    "\n",
    "    ##selenium\n",
    "    # Pricyness (in 1-5) (SHOULD): pricyness\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(page)\n",
    "    soup_s = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    info =[]\n",
    "\n",
    "    try:\n",
    "        for s in soup_s.select(\"script[type='application/ld+json']\"):\n",
    "            home_url = json.loads(s.get_text(strip=True))\n",
    "            info.append(home_url)\n",
    "        restaurant_info['pricyness'] = info[0]['priceRange']\n",
    "\n",
    "        # Latitude (introduce later) (MUST): latitude (generated during cleaning)\n",
    "        restaurant_info['latitude'] = info[0]['geo']['latitude']\n",
    "\n",
    "        # Longitude (introduce later) (MUST): longitude (generated during cleaning)\n",
    "        restaurant_info['longitude'] = info[0]['geo']['longitude']\n",
    "\n",
    "    except Exception:\n",
    "        restaurant_info['pricyness'] = None\n",
    "        restaurant_info['latitude'] = None\n",
    "        restaurant_info['longitude'] = None\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Average delivery time (COULD): avg_delivery_time\n",
    "    try:\n",
    "        restaurant_info['avg_delivery_time'] = soup.find_all(class_=\"DeliveryInfo__DeliveryInformationText-sc-8y92dv-2 dYkBpD\")[0].find(\"strong\").text\n",
    "    except Exception:\n",
    "        restaurant_info['avg_delivery_time'] =  None\n",
    "\n",
    "    # save_in_csv(restaurant_info)\n",
    "    save_in_csv(restaurant_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf18b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_scraper(all_links):\n",
    "    i = 0\n",
    "    for key, values in all_links.items():\n",
    "        for link in values:\n",
    "            fetch_page(link)\n",
    "            time.sleep(random.randrange(1, 100) / 100)\n",
    "        i += 1\n",
    "        print(f\" Scraped {key} zip code ({i}/194)..............\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03bb242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = all_links_by_zipcode(zip_code_list[0:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68524df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all_links into a csv\n",
    "with open(\"../geotracker/data/links_list_2611.csv\", 'a') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(all_links.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc7482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scraped 10117 zip code (1/194)..............\n",
      " Scraped 10115 zip code (2/194)..............\n",
      " Scraped 10119 zip code (3/194)..............\n",
      " Scraped 10178 zip code (4/194)..............\n",
      " Scraped 10179 zip code (5/194)..............\n",
      " Scraped 10243 zip code (6/194)..............\n",
      " Scraped 10245 zip code (7/194)..............\n",
      " Scraped 10247 zip code (8/194)..............\n",
      " Scraped 10405 zip code (9/194)..............\n",
      " Scraped 10435 zip code (10/194)..............\n",
      " Scraped 10437 zip code (11/194)..............\n",
      " Scraped 10587 zip code (12/194)..............\n",
      " Scraped 10623 zip code (13/194)..............\n",
      " Scraped 10707 zip code (14/194)..............\n",
      " Scraped 10719 zip code (15/194)..............\n",
      " Scraped 10785 zip code (16/194)..............\n",
      " Scraped 10787 zip code (17/194)..............\n",
      " Scraped 10961 zip code (18/194)..............\n",
      " Scraped 10963 zip code (19/194)..............\n",
      " Scraped 10969 zip code (20/194)..............\n",
      " Scraped 10997 zip code (21/194)..............\n",
      " Scraped 10999 zip code (22/194)..............\n",
      " Scraped 11011 zip code (23/194)..............\n",
      " Scraped 12489 zip code (24/194)..............\n",
      " Scraped 12623 zip code (25/194)..............\n",
      " Scraped 13127 zip code (26/194)..............\n",
      " Scraped 13353 zip code (27/194)..............\n",
      " Scraped 14195 zip code (28/194)..............\n",
      " Scraped 10249 zip code (29/194)..............\n",
      " Scraped 10315 zip code (30/194)..............\n",
      " Scraped 10317 zip code (31/194)..............\n",
      " Scraped 10318 zip code (32/194)..............\n",
      " Scraped 10319 zip code (33/194)..............\n",
      " Scraped 10365 zip code (34/194)..............\n",
      " Scraped 10367 zip code (35/194)..............\n",
      " Scraped 10369 zip code (36/194)..............\n",
      " Scraped 10407 zip code (37/194)..............\n",
      " Scraped 10409 zip code (38/194)..............\n",
      " Scraped 10439 zip code (39/194)..............\n",
      " Scraped 10551 zip code (40/194)..............\n",
      " Scraped 10553 zip code (41/194)..............\n",
      " Scraped 10555 zip code (42/194)..............\n",
      " Scraped 10557 zip code (43/194)..............\n",
      " Scraped 10559 zip code (44/194)..............\n",
      " Scraped 10585 zip code (45/194)..............\n"
     ]
    }
   ],
   "source": [
    "start_scraper(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07561a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deberia haber limpiado las listas de links de repetidos!!!!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "104bebf55b610e28190aa34649b8baa71cbfc6406c1ff71f682515b00fd61d0c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('geotracker': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
